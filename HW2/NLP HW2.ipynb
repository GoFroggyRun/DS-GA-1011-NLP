{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = [float(i) for i in tokens[1:]]\n",
    "    return data\n",
    "\n",
    "vecs = load_vectors('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vecs['<pad>'] = list(np.zeros((300)))\n",
    "vecs['<unk>'] = list(np.random.normal(scale=0.4, size=(300, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('snli_train.tsv', delimiter='\\t')\n",
    "premise = []\n",
    "hyp = []\n",
    "target = []\n",
    "for i in range(len(train)):\n",
    "    premise.append(train.iloc[i]['sentence1'].split())\n",
    "    hyp.append(train.iloc[i]['sentence2'].split())\n",
    "    target.append(train.iloc[i]['label'])\n",
    "    \n",
    "label_dcit = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "labels = []\n",
    "for label in target:\n",
    "    labels.append(label_dcit[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('snli_val.tsv', delimiter='\\t')\n",
    "premise_val = []\n",
    "hyp_val = []\n",
    "target_val = []\n",
    "for i in range(len(valid)):\n",
    "    premise_val.append(valid.iloc[i]['sentence1'].split())\n",
    "    hyp_val.append(valid.iloc[i]['sentence2'].split())\n",
    "    target_val.append(valid.iloc[i]['label'])\n",
    "    \n",
    "label_dcit = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "labels_val = []\n",
    "for label in target_val:\n",
    "    labels_val.append(label_dcit[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def build_vocab(sentences):\n",
    "    # create vocab of words\n",
    "    max_sent_len = max([len(sent[0]) for sent in sentences])\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_set = list(set(word_dict))\n",
    "    id2char = word_set\n",
    "    char2id = dict(zip(word_set, range(2,2+len(word_set))))\n",
    "    id2char = ['<pad>', '<unk>'] + id2char\n",
    "    char2id['<pad>'] = PAD_IDX\n",
    "    char2id['<unk>'] = UNK_IDX\n",
    "    return char2id, id2char, max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id, id2char, max_sent_len = build_vocab(sentences = premise + hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s1, s2, label, char2id):\n",
    "\n",
    "        self.s1, self.s2, self.target_list = s1, s2, label\n",
    "        assert (len(self.s1) == len(self.target_list) == len(self.s2))\n",
    "        self.char2id = char2id\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.s1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        s1_idx = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.s1[key][:max_sent_len]]\n",
    "        s2_idx = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.s2[key][:max_sent_len]]\n",
    "        label = self.target_list[key]\n",
    "        return [s1_idx, len(s1_idx),s2_idx, len(s2_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    s1 = []\n",
    "    s1_len = []\n",
    "    s2 = []\n",
    "    s2_len = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        s1_len.append(datum[1])\n",
    "        s2_len.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,max_sent_len-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_s2 = np.pad(np.array(datum[2]),\n",
    "                                pad_width=((0,max_sent_len-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1.append(padded_vec_s1)\n",
    "        s2.append(padded_vec_s2)\n",
    "    \n",
    "    ind_s1 = np.argsort(s1_len)[::-1]\n",
    "    ind_s2 = np.argsort(s2_len)[::-1]\n",
    "    s1 = np.array(s1)[ind_s1]\n",
    "    s1_len = np.array(s1_len)[ind_s1]\n",
    "    s2 = np.array(s2)[ind_s2]\n",
    "    s2_len = np.array(s2_len)[ind_s2]\n",
    "    label_list = np.array(label_list)\n",
    "    return [torch.from_numpy(np.array(s1)), torch.LongTensor(s1_len), ind_s1, torch.from_numpy(np.array(s2)), torch.LongTensor(s2_len), ind_s2, torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set =VocabDataset(premise, hyp, labels, char2id)\n",
    "val_set =VocabDataset(premise_val, hyp_val, labels_val, char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(perm):\n",
    "    inverse = [1] * len(perm)\n",
    "    for index, perm in enumerate(perm):\n",
    "        inverse[perm] = index\n",
    "    return inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_emb_layer(batch_data, cold_w2v, emb_size = 300):\n",
    "    embed = np.zeros((len(batch_data),max_sent_len, emb_size))\n",
    "    for i in range(len(batch_data)):\n",
    "        for j in range(len(batch_data[i])):\n",
    "            try:\n",
    "                embed[i,j,:] = np.array(cold_w2v[id2char[batch_data[i][j]]])\n",
    "            except KeyError:\n",
    "                embed[i,j,:] = np.array(cold_w2v['<unk>'])\n",
    "    return torch.from_numpy(embed).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, emb_size=300):\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        super(BiGRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, data, lengths, sort_idx):\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed = prep_emb_layer(data, vecs)\n",
    "        embed = embed.to(device)\n",
    "        # pack padded sequence\n",
    "        sent_packed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        # fprop though GRU\n",
    "        _, hn = self.gru(sent_packed)\n",
    "        cat = torch.cat((hn[0], hn[1]), 1)\n",
    "        # unsort \n",
    "        reverse_idx = inverse(sort_idx)\n",
    "        un_sort = cat.index_select(0, torch.LongTensor(reverse_idx).to(device))\n",
    "\n",
    "        return un_sort\n",
    "\n",
    "class Fc_GRU(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, Fc_dim, dropout=False, interact=False):\n",
    "        super(Fc_GRU, self).__init__()\n",
    "\n",
    "        self.encoder = BiGRU(hidden_size, num_layers, num_classes, emb_size =300)\n",
    "        self.interact = interact\n",
    "        inputdim = 4*hidden_size\n",
    "        if interact:\n",
    "            inputdim = 6*hidden_size\n",
    "\n",
    "        if not dropout:\n",
    "            self.fc = nn.Sequential(\n",
    "                    nn.Linear(inputdim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(Fc_dim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(Fc_dim, num_classes)\n",
    "                    )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(inputdim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(Fc_dim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(Fc_dim, num_classes)\n",
    "                    )\n",
    "    def forward(self, s1, l1, idx1, s2, l2, idx2):\n",
    "        s1 = self.encoder(s1, l1, idx1)\n",
    "        s2 = self.encoder(s2, l2, idx2)\n",
    "\n",
    "        if not self.interact:\n",
    "            cat = torch.cat((s1, s2), 1)\n",
    "        else:\n",
    "            cat = torch.cat((s1, s1*s2, s2), 1)\n",
    "        logits = self.fc(cat)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def test_GRU_model(data, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0 \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for (data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in data:\n",
    "        data_1,data_2, label = \\\n",
    "            data_1.to(device),data_2.to(device), label.to(device)\n",
    "        outputs = F.softmax(model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total_loss += criterion(outputs, label).item()\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_hidden_size = {}\n",
    "train_loss_hidden_size = {}\n",
    "val_acc_hidden_size = {}\n",
    "val_loss_hidden_size = {}\n",
    "num_pars = {}\n",
    "for hid_size in [100, 200, 800, 1600]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_GRU(hidden_size=hid_size, num_layers=1, num_classes=3, Fc_dim = 300, dropout=False)\n",
    "    num_pars[hid_size] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_GRU_model(val_loader , model)\n",
    "        train_acc, train_loss =test_GRU_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_hidden_size[hid_size] = epoch_loss_train\n",
    "    val_loss_hidden_size[hid_size] = epoch_loss_val\n",
    "    train_acc_hidden_size[hid_size] = epoch_acc_train\n",
    "    val_acc_hidden_size[hid_size] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( train_acc_hidden_size, open( \"train_acc_GRU_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_hidden_size, open( \"val_acc_GRU_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_hidden_size, open( \"train_loss_GRU_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_hidden_size, open( \"val_loss_GRU_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars, open( \"num_pars_GRU_hdn.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_drop = {}\n",
    "train_loss_drop = {}\n",
    "val_acc_drop = {}\n",
    "val_loss_drop = {}\n",
    "num_pars_drop = {}\n",
    "for drop in [True, False]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=drop)\n",
    "    num_pars_drop[drop] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_GRU_model(val_loader , model)\n",
    "        train_acc, train_loss =test_GRU_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_drop[drop] = epoch_loss_train\n",
    "    val_loss_drop[drop] = epoch_loss_val\n",
    "    train_acc_drop[drop] = epoch_acc_train\n",
    "    val_acc_drop[drop] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_drop, open( \"train_acc_GRU_dp.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_drop, open( \"val_acc_GRU_dp.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_drop, open( \"train_loss_GRU_dp.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_drop, open( \"val_loss_GRU_dp.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_drop, open( \"num_pars_GRU_dp.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_decay = {}\n",
    "train_loss_decay = {}\n",
    "val_acc_decay = {}\n",
    "val_loss_decay = {}\n",
    "num_pars_decay = {}\n",
    "for decay in [1e-6, 2e-5, 2e-4]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True)\n",
    "    num_pars_decay[decay] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = decay)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_GRU_model(val_loader , model)\n",
    "        train_acc, train_loss =test_GRU_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_decay[decay] = epoch_loss_train\n",
    "    val_loss_decay[decay] = epoch_loss_val\n",
    "    train_acc_decay[decay] = epoch_acc_train\n",
    "    val_acc_decay[decay] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_decay, open( \"train_acc_GRU_decay.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_decay, open( \"val_acc_GRU_decay.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_decay, open( \"train_loss_GRU_decay.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_decay, open( \"val_loss_GRU_decay.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_decay, open( \"num_pars_GRU_decay.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_int = {}\n",
    "train_loss_int = {}\n",
    "val_acc_int = {}\n",
    "val_loss_int = {}\n",
    "num_pars_int = {}\n",
    "for interact in [True, False]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=interact)\n",
    "    num_pars_int[interact] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-6)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_GRU_model(val_loader , model)\n",
    "        train_acc, train_loss =test_GRU_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_int[interact] = epoch_loss_train\n",
    "    val_loss_int[interact] = epoch_loss_val\n",
    "    train_acc_int[interact] = epoch_acc_train\n",
    "    val_acc_int[interact] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_int, open( \"train_acc_GRU_int.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_int, open( \"val_acc_GRU_int.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_int, open( \"train_loss_GRU_int.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_int, open( \"val_loss_GRU_int.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_int, open( \"num_pars_GRU_int.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True: [62.3,\n",
       "  65.2,\n",
       "  68.0,\n",
       "  70.1,\n",
       "  70.9,\n",
       "  71.2,\n",
       "  70.0,\n",
       "  70.6,\n",
       "  69.9,\n",
       "  70.3,\n",
       "  71.6,\n",
       "  72.8,\n",
       "  70.8,\n",
       "  70.3,\n",
       "  71.2],\n",
       " False: [64.3,\n",
       "  68.3,\n",
       "  69.5,\n",
       "  69.4,\n",
       "  71.3,\n",
       "  72.4,\n",
       "  70.8,\n",
       "  71.7,\n",
       "  71.6,\n",
       "  69.6,\n",
       "  70.5,\n",
       "  70.3,\n",
       "  70.1,\n",
       "  70.1,\n",
       "  68.8]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers = 2, k_size = 3, num_classes =3 , emb_size =300):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(emb_size, hidden_size, kernel_size=k_size, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=k_size, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, idx):\n",
    "        batch_size, seq_len = x.size()\n",
    "        reverse_idx = inverse(idx)\n",
    "        x = x[reverse_idx]\n",
    "        embed = prep_emb_layer(x, vecs)\n",
    "        embed = embed.to(device)\n",
    "        embed = embed.transpose(1,2).contiguous()\n",
    "        hidden = self.conv1(embed)        \n",
    "        hidden = self.conv2(hidden)        \n",
    "        return torch.max(hidden, 2)[0]\n",
    "\n",
    "class Fc_CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, Fc_dim, k_size = 3, num_classes =3, dropout=False, interact = False):\n",
    "        super(Fc_CNN, self).__init__()\n",
    "        self.interact = interact\n",
    "        self.encoder = CNN(hidden_size, num_layers, k_size = k_size)\n",
    "        inputdim = 2*hidden_size\n",
    "        if interact:\n",
    "            inputdim = 3*hidden_size\n",
    "        if not dropout:\n",
    "            self.fc = nn.Sequential(\n",
    "                    nn.Linear(inputdim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(Fc_dim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(Fc_dim, num_classes)\n",
    "                    )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(inputdim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(Fc_dim, Fc_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(Fc_dim, num_classes)\n",
    "                    )\n",
    "\n",
    "    def forward(self, s1, idx1, s2, idx2):\n",
    "        s1 = self.encoder(s1, idx1)\n",
    "        s2 = self.encoder(s2, idx2)\n",
    "        if self.interact:\n",
    "            cat = torch.cat((s1,s1*s2, s2), 1)\n",
    "        else:\n",
    "            cat = torch.cat((s1, s2), 1)\n",
    "        logits = self.fc(cat)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CNN_model(data, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0 \n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for (data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in data:\n",
    "        data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "        outputs = F.softmax(model(data_1, idx_1, data_2, idx_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total_loss += criterion(outputs, label).item()\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_hidden_size_cnn = {}\n",
    "train_loss_hidden_size_cnn = {}\n",
    "val_acc_hidden_size_cnn = {}\n",
    "val_loss_hidden_size_cnn = {}\n",
    "num_pars_cnn = {}\n",
    "for hid_size in [100, 300, 1200, 2400]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_CNN(hidden_size=hid_size, num_layers=2, num_classes=3, Fc_dim = 300, dropout=False)\n",
    "    num_pars_cnn[hid_size] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, idx_1, data_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_CNN_model(val_loader , model)\n",
    "        train_acc, train_loss =test_CNN_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_hidden_size_cnn[hid_size] = epoch_loss_train\n",
    "    val_loss_hidden_size_cnn[hid_size] = epoch_loss_val\n",
    "    train_acc_hidden_size_cnn[hid_size] = epoch_acc_train\n",
    "    val_acc_hidden_size_cnn[hid_size] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_hidden_size_cnn, open( \"train_acc_CNN_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_hidden_size_cnn, open( \"val_acc_CNN_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_hidden_size_cnn, open( \"train_loss_CNN_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_hidden_size_cnn, open( \"val_loss_CNN_hdn.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_cnn, open( \"num_pars_CNN_hdn.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: [62.6,\n",
       "  65.9,\n",
       "  66.3,\n",
       "  68.1,\n",
       "  69.6,\n",
       "  67.7,\n",
       "  69.0,\n",
       "  67.9,\n",
       "  67.7,\n",
       "  69.1,\n",
       "  69.5,\n",
       "  68.3,\n",
       "  67.8,\n",
       "  68.2,\n",
       "  68.6],\n",
       " 300: [64.9,\n",
       "  67.0,\n",
       "  71.0,\n",
       "  70.6,\n",
       "  69.1,\n",
       "  70.9,\n",
       "  68.2,\n",
       "  68.0,\n",
       "  69.8,\n",
       "  67.6,\n",
       "  70.0,\n",
       "  67.7,\n",
       "  67.1,\n",
       "  66.6,\n",
       "  67.9],\n",
       " 1200: [66.2,\n",
       "  67.5,\n",
       "  71.1,\n",
       "  70.9,\n",
       "  69.1,\n",
       "  69.6,\n",
       "  70.4,\n",
       "  70.6,\n",
       "  69.4,\n",
       "  70.1,\n",
       "  70.2,\n",
       "  70.1,\n",
       "  68.4,\n",
       "  70.0,\n",
       "  70.4],\n",
       " 2400: [66.3,\n",
       "  69.4,\n",
       "  68.9,\n",
       "  71.3,\n",
       "  71.5,\n",
       "  70.6,\n",
       "  68.5,\n",
       "  69.5,\n",
       "  70.2,\n",
       "  69.8,\n",
       "  69.6,\n",
       "  71.1,\n",
       "  71.0,\n",
       "  71.4,\n",
       "  69.5]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_hidden_size_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_k_size_cnn = {}\n",
    "train_loss_k_size_cnn = {}\n",
    "val_acc_k_size_cnn = {}\n",
    "val_loss_k_size_cnn = {}\n",
    "num_pars_k_size_cnn = {}\n",
    "for k_size in [1, 2, 3]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = k_size, dropout=False)\n",
    "    num_pars_k_size_cnn[k_size] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, idx_1, data_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_CNN_model(val_loader , model)\n",
    "        train_acc, train_loss =test_CNN_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_k_size_cnn[k_size] = epoch_loss_train\n",
    "    val_loss_k_size_cnn[k_size] = epoch_loss_val\n",
    "    train_acc_k_size_cnn[k_size] = epoch_acc_train\n",
    "    val_acc_k_size_cnn[k_size] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_k_size_cnn, open( \"train_acc_CNN_ker.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_k_size_cnn, open( \"val_acc_CNN_ker.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_k_size_cnn, open( \"train_loss_CNN_ker.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_k_size_cnn, open( \"val_loss_CNN_ker.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_k_size_cnn, open( \"num_pars_CNN_ker.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [66.6,\n",
       "  70.3,\n",
       "  70.4,\n",
       "  73.6,\n",
       "  72.2,\n",
       "  70.8,\n",
       "  72.3,\n",
       "  72.4,\n",
       "  74.2,\n",
       "  73.6,\n",
       "  71.0,\n",
       "  71.6,\n",
       "  71.8,\n",
       "  71.5,\n",
       "  69.9],\n",
       " 2: [67.4,\n",
       "  69.2,\n",
       "  70.4,\n",
       "  70.7,\n",
       "  71.0,\n",
       "  70.7,\n",
       "  71.0,\n",
       "  69.9,\n",
       "  68.8,\n",
       "  69.0,\n",
       "  70.6,\n",
       "  68.9,\n",
       "  68.8,\n",
       "  69.4,\n",
       "  69.4],\n",
       " 3: [67.1,\n",
       "  67.6,\n",
       "  69.5,\n",
       "  70.3,\n",
       "  68.5,\n",
       "  68.5,\n",
       "  69.2,\n",
       "  69.7,\n",
       "  69.3,\n",
       "  70.1,\n",
       "  68.7,\n",
       "  68.2,\n",
       "  67.8,\n",
       "  68.1,\n",
       "  70.4]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_k_size_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_int_cnn = {}\n",
    "train_loss_int_cnn = {}\n",
    "val_acc_int_cnn = {}\n",
    "val_loss_int_cnn = {}\n",
    "num_pars_int_cnn = {}\n",
    "for interact in [True, False]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = 1, dropout=False, interact = interact)\n",
    "    num_pars_int_cnn[interact] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, idx_1, data_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_CNN_model(val_loader , model)\n",
    "        train_acc, train_loss =test_CNN_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_int_cnn[interact] = epoch_loss_train\n",
    "    val_loss_int_cnn[interact] = epoch_loss_val\n",
    "    train_acc_int_cnn[interact] = epoch_acc_train\n",
    "    val_acc_int_cnn[interact] = epoch_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_int_cnn, open( \"train_acc_CNN_int.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_int_cnn, open( \"val_acc_CNN_int.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_int_cnn, open( \"train_loss_CNN_int.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_int_cnn, open( \"val_loss_CNN_int.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_int_cnn, open( \"num_pars_CNN_int.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 15 done!\n",
      "Epoch 16 done!\n",
      "Epoch 17 done!\n",
      "Epoch 18 done!\n",
      "Epoch 19 done!\n",
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n",
      "Epoch 15 done!\n",
      "Epoch 16 done!\n",
      "Epoch 17 done!\n",
      "Epoch 18 done!\n",
      "Epoch 19 done!\n"
     ]
    }
   ],
   "source": [
    "train_acc_reg_cnn = {}\n",
    "train_loss_reg_cnn = {}\n",
    "val_acc_reg_cnn = {}\n",
    "val_loss_reg_cnn = {}\n",
    "num_pars_reg_cnn = {}\n",
    "for reg in [True, False]:\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = 1, dropout=reg, interact = True)\n",
    "    num_pars_reg_cnn[reg] = count_parameters(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 20\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-6)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, idx_1, data_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_CNN_model(val_loader , model)\n",
    "        train_acc, train_loss =test_CNN_model(train_loader , model)\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    train_loss_reg_cnn[reg] = epoch_loss_train\n",
    "    val_loss_reg_cnn[reg] = epoch_loss_val\n",
    "    train_acc_reg_cnn[reg] = epoch_acc_train\n",
    "    val_acc_reg_cnn[reg] = epoch_acc_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_acc_reg_cnn, open( \"train_acc_CNN_reg.p\", \"wb\" ) )\n",
    "pickle.dump( val_acc_reg_cnn, open( \"val_acc_CNN_reg.p\", \"wb\" ) )\n",
    "pickle.dump( train_loss_reg_cnn, open( \"train_loss_CNN_reg.p\", \"wb\" ) )\n",
    "pickle.dump( val_loss_reg_cnn, open( \"val_loss_CNN_reg.p\", \"wb\" ) )\n",
    "pickle.dump( num_pars_reg_cnn, open( \"num_pars_CNN_reg.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True: [64.6,\n",
       "  68.4,\n",
       "  68.5,\n",
       "  71.3,\n",
       "  70.9,\n",
       "  71.3,\n",
       "  73.7,\n",
       "  72.9,\n",
       "  74.2,\n",
       "  73.5,\n",
       "  74.5,\n",
       "  74.6,\n",
       "  74.1,\n",
       "  75.3,\n",
       "  73.7,\n",
       "  74.8,\n",
       "  74.9,\n",
       "  74.5,\n",
       "  75.7,\n",
       "  74.2],\n",
       " False: [68.8,\n",
       "  73.0,\n",
       "  74.0,\n",
       "  76.6,\n",
       "  77.1,\n",
       "  78.7,\n",
       "  78.0,\n",
       "  77.9,\n",
       "  76.0,\n",
       "  75.6,\n",
       "  76.8,\n",
       "  76.0,\n",
       "  75.7,\n",
       "  75.7,\n",
       "  76.3,\n",
       "  75.0,\n",
       "  74.9,\n",
       "  75.9,\n",
       "  74.3,\n",
       "  75.0]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_reg_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_BiGru():\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-6)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_GRU_model(val_loader , model)\n",
    "        train_acc, train_loss =test_GRU_model(train_loader , model)\n",
    "        try:\n",
    "            if val_acc > max(epoch_acc_val):\n",
    "                torch.save(model.state_dict(), 'my_BiGRU.pth')\n",
    "        except ValueError:\n",
    "            val_acc = val_acc\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    return epoch_acc_val, epoch_acc_train, epoch_loss_val, epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "my_Gru_acc_val, my_Gru_acc_train, my_Gru_los_val, my_Gru_los_train = find_best_BiGru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_best_gru = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=True)\n",
    "my_best_gru = my_best_gru.to(device)\n",
    "my_best_gru.load_state_dict(torch.load('my_BiGRU.pth'))\n",
    "san_check, _ = test_GRU_model(val_loader , my_best_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63.0,\n",
       " 67.6,\n",
       " 69.1,\n",
       " 70.0,\n",
       " 71.3,\n",
       " 71.6,\n",
       " 72.8,\n",
       " 72.7,\n",
       " 73.5,\n",
       " 72.7,\n",
       " 72.6,\n",
       " 72.9,\n",
       " 72.7,\n",
       " 72.1,\n",
       " 73.3]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Gru_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_CNN():\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_val = []\n",
    "    epoch_loss_val = []\n",
    "    model = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = 1, dropout=False, interact = True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 15\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-6)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "            data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_1, idx_1, data_2, idx_2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_acc, val_loss = test_CNN_model(val_loader , model)\n",
    "        train_acc, train_loss =test_CNN_model(train_loader , model)\n",
    "        try:\n",
    "            if val_acc > max(epoch_acc_val):\n",
    "                torch.save(model.state_dict(), 'my_CNN.pth')\n",
    "        except ValueError:\n",
    "            torch.save(model.state_dict(), 'my_CNN.pth')\n",
    "        epoch_acc_val.append(val_acc)\n",
    "        epoch_acc_train.append(train_acc)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        print(\"Epoch {} done!\".format(epoch))\n",
    "    epoch_acc_val.append(val_acc)\n",
    "    epoch_acc_train.append(train_acc)\n",
    "    epoch_loss_val.append(val_loss)\n",
    "    epoch_loss_train.append(train_loss)\n",
    "    return epoch_acc_val, epoch_acc_train, epoch_loss_val, epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done!\n",
      "Epoch 1 done!\n",
      "Epoch 2 done!\n",
      "Epoch 3 done!\n",
      "Epoch 4 done!\n",
      "Epoch 5 done!\n",
      "Epoch 6 done!\n",
      "Epoch 7 done!\n",
      "Epoch 8 done!\n",
      "Epoch 9 done!\n",
      "Epoch 10 done!\n",
      "Epoch 11 done!\n",
      "Epoch 12 done!\n",
      "Epoch 13 done!\n",
      "Epoch 14 done!\n"
     ]
    }
   ],
   "source": [
    "my_cnn_acc_val, my_cnn_acc_train, my_cnn_los_val, my_cnn_los_train = find_best_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_best_cnn = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = 1, dropout=False, interact = True)\n",
    "my_best_cnn = my_best_cnn.to(device)\n",
    "my_best_cnn.load_state_dict(torch.load('my_CNN.pth'))\n",
    "san_check, _ = test_CNN_model(val_loader , my_best_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_GRU_predictions(loader, model, m_type= 'GRU'):\n",
    "    # use this function to print prediction samples\n",
    "    correct = {}\n",
    "    wrong = {}\n",
    "    model.eval()\n",
    "    for data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label in loader:\n",
    "        data_1,data_2, label = \\\n",
    "            data_1.to(device),data_2.to(device), label.to(device)\n",
    "        if m_type == 'GRU':\n",
    "            outputs = F.softmax(model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2), dim=1)\n",
    "        elif m_type == 'CNN':\n",
    "            outputs = F.softmax(model(data_1, idx_1, data_2, idx_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        for ind in range(32):\n",
    "            # record correct indices data to the correct file\n",
    "            if (predicted.tolist()[ind][0] == label.data.tolist()[ind]):\n",
    "                correct[ind] = {}\n",
    "                correct[ind]['s1'] = (data_1[ind].tolist())\n",
    "                correct[ind]['s2'] = (data_2[ind].tolist())\n",
    "                correct[ind]['lb'] = (label[ind].tolist())\n",
    "            else:\n",
    "                wrong[ind] = {}\n",
    "                wrong[ind]['s1'] = (data_1[ind].tolist())\n",
    "                wrong[ind]['s2'] = (data_2[ind].tolist())\n",
    "                wrong[ind]['lb'] = (label[ind].tolist())\n",
    "        # stop when we have enough data\n",
    "        if (len(correct) > 3) and (len(wrong) > 3):\n",
    "            break\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_cor, GRU_wro = print_GRU_predictions(val_loader , my_best_gru, 'GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_cor, CNNwro = print_GRU_predictions(val_loader , my_best_cnn, 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_sentence(meta):\n",
    "    s1, s2 = [], []\n",
    "# extract meaningful word indices\n",
    "    for id in meta['s1']:\n",
    "        # stop at padding indices\n",
    "        if id == 0:\n",
    "            break\n",
    "        # transform indices to text\n",
    "        s1.append(id2char[id])\n",
    "    for id in meta['s2']:\n",
    "        # stop at padding indices\n",
    "        if id == 0:\n",
    "            break\n",
    "        # transform indices to text\n",
    "        s2.append(id2char[id])\n",
    "    return [' '.join(s1)], [' '.join(s2)], ('entitlement' if meta['lb']==0 else ( 'neutral' if meta['lb']==1 else 'contradiction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A building that <unk> beautiful architecture stands in the sunlight as somebody on a bike passes by'], ['Three sisters , barefoot in pink dresses and who range in age from preschool to teenager'], 'contradiction')\n",
      "(['A woman is looking at a pamphlet talking to someone , while a guy takes photos in'], ['The boy in the black and red swimsuit is about to go swimming'], 'entitlement')\n",
      "(['A blond-haired lady with a gray tank top and jeans is holding her phone while she puts'], ['A woman waits for her husband to come home for dinner .'], 'entitlement')\n"
     ]
    }
   ],
   "source": [
    "for (count,idx) in enumerate(GRU_cor):\n",
    "    print(id_2_sentence(GRU_cor[idx]))\n",
    "    if count == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A gentleman in a striped shirt gesturing with a <unk> object in his hand while passersby stare'], ['A group of dancers is about to dance for a school competition'], 'entitlement')\n",
      "(['Two boys are swimming underwater in a pool and a girl is swimming in the background .'], ['A man is talking to his wife on his cellphone .'], 'contradiction')\n",
      "(['A barefooted adolescent boy in a yellow shirt reaching the top of a small skateboarding ramp .'], ['No one is sliding down a water slide .'], 'neutral')\n"
     ]
    }
   ],
   "source": [
    "for (count,idx) in enumerate(GRU_wro):\n",
    "    print(id_2_sentence(GRU_wro[idx]))\n",
    "    if count == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A group of dancers with green shirts are all holding hands in a circle , one lady'], ['A child is playing with a toy airplane that was made by his grandfather .'], 'neutral')\n",
      "(['A man in a blue shirt and blue jeans rides a dark brown horse with white feet'], ['A live band on a lawn jamming out for the holiday crowd .'], 'neutral')\n",
      "(['A man who has a gray beard and gray hair laughs while wearing a purple shirt .'], ['A fire engine shoots water at a young man with an umbrella .'], 'entitlement')\n"
     ]
    }
   ],
   "source": [
    "for (count,idx) in enumerate(CNN_cor):\n",
    "    print(id_2_sentence(CNN_cor[idx]))\n",
    "    if count == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A woman , wearing a white shirt and green shorts , sitting on a rock in a'], ['A man is laughing at a woman who has fallen over .'], 'neutral')\n",
      "(['A young man shielding himself from a stream of water from a fire engine by using an'], ['A group of dancers is about to dance for a school competition'], 'contradiction')\n",
      "(['A person with dark hair is standing on the sidewalk in front of an orange and white'], ['A boy is waiting in line for the Ferris Wheel .'], 'entitlement')\n"
     ]
    }
   ],
   "source": [
    "for (count,idx) in enumerate(CNNwro):\n",
    "    print(id_2_sentence(CNNwro[idx]))\n",
    "    if count == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_val = pd.read_csv('mnli_val.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_p_genre(model='GRU', genre_data = mul_val):\n",
    "    gens = genre_data['genre'].unique()\n",
    "    gen_acc = {}\n",
    "    for gen in gens:\n",
    "        gen_data = (genre_data[genre_data['genre'] == gen]).drop(['genre'], axis=1)\n",
    "        premise = []\n",
    "        hyp = []\n",
    "        target = []\n",
    "        for i in range(len(gen_data)):\n",
    "            premise.append(gen_data.iloc[i]['sentence1'].split())\n",
    "            hyp.append(gen_data.iloc[i]['sentence2'].split())\n",
    "            target.append(gen_data.iloc[i]['label'])\n",
    "        label_dcit = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "        labels = []\n",
    "        for label in target:\n",
    "            labels.append(label_dcit[label])\n",
    "        token = VocabDataset(premise, hyp, labels, char2id)\n",
    "        gen_loader = torch.utils.data.DataLoader(dataset=token, batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=collate_func, shuffle=True)\n",
    "        if model == 'GRU':\n",
    "            my_mod = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=True)\n",
    "            my_mod = my_best_gru.to(device)\n",
    "            my_mod.load_state_dict(torch.load('my_BiGRU.pth'))\n",
    "            acc, _ = test_GRU_model(gen_loader , my_mod)            \n",
    "        elif model == 'CNN':\n",
    "            my_mod = Fc_CNN(hidden_size=1200, num_layers=2, num_classes=3, Fc_dim = 300, k_size = 1, dropout=False, interact = True)\n",
    "            my_mod = my_best_cnn.to(device)\n",
    "            my_mod.load_state_dict(torch.load('my_CNN.pth'))\n",
    "            acc, _ = test_CNN_model(gen_loader , my_mod)\n",
    "            \n",
    "        gen_acc[gen] = acc\n",
    "    return gen_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_genre = get_acc_p_genre('GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': 49.447236180904525,\n",
       " 'telephone': 44.37810945273632,\n",
       " 'slate': 46.007984031936125,\n",
       " 'government': 44.58661417322835,\n",
       " 'travel': 45.010183299389}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRU_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_genre = get_acc_p_genre('CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': 41.50753768844221,\n",
       " 'telephone': 42.18905472636816,\n",
       " 'slate': 38.82235528942116,\n",
       " 'government': 39.76377952755905,\n",
       " 'travel': 37.78004073319756}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_train = pd.read_csv('mnli_train.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_GRU_p_genre(multi_train, multi_val):\n",
    "    gens = multi_train['genre'].unique()\n",
    "    gen_acc = {}\n",
    "    for gen in gens:\n",
    "        train_data = (multi_train[multi_train['genre'] == gen]).drop(['genre'], axis=1)\n",
    "        premise = []\n",
    "        hyp = []\n",
    "        target = []\n",
    "        for i in range(len(train_data)):\n",
    "            premise.append(train_data.iloc[i]['sentence1'].split())\n",
    "            hyp.append(train_data.iloc[i]['sentence2'].split())\n",
    "            target.append(train_data.iloc[i]['label'])\n",
    "        label_dcit = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "        labels = []\n",
    "        for label in target:\n",
    "            labels.append(label_dcit[label])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=VocabDataset(premise, hyp, labels, char2id), \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=collate_func, shuffle=True)\n",
    "        \n",
    "        val_data = (multi_val[multi_val['genre'] == gen]).drop(['genre'], axis=1)\n",
    "        premise = []\n",
    "        hyp = []\n",
    "        target = []\n",
    "        for i in range(len(val_data)):\n",
    "            premise.append(val_data.iloc[i]['sentence1'].split())\n",
    "            hyp.append(val_data.iloc[i]['sentence2'].split())\n",
    "            target.append(val_data.iloc[i]['label'])\n",
    "        labels = []\n",
    "        for label in target:\n",
    "            labels.append(label_dcit[label])\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=VocabDataset(premise, hyp, labels, char2id), \n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=collate_func, shuffle=True)\n",
    "        \n",
    "        epoch_acc_val = []\n",
    "        model = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=True)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(torch.load('my_BiGRU.pth'))\n",
    "\n",
    "        learning_rate = 5e-5\n",
    "        num_epochs = 20\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-6)\n",
    "        total_step = len(train_loader)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i,(data_1, lengths_1, idx_1,data_2, lengths_2, idx_2, label) in enumerate(train_loader):\n",
    "                data_1,data_2, label = data_1.to(device),data_2.to(device), label.to(device)\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                outputs = model(data_1, lengths_1, idx_1, data_2, lengths_2, idx_2)\n",
    "                loss = criterion(outputs, label)\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            val_acc, _ = test_GRU_model(val_loader , model)\n",
    "            try:\n",
    "                if val_acc > max(epoch_acc_val):\n",
    "                    torch.save(model.state_dict(), 'best_GRU_4_' + gen + '.pth')\n",
    "            except ValueError:\n",
    "                torch.save(model.state_dict(), 'best_GRU_4_' + gen + '.pth')\n",
    "            epoch_acc_val.append(val_acc)\n",
    "        gen_acc[gen] = epoch_acc_val\n",
    "    return gen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_res= fine_tune_GRU_p_genre(mul_train, mul_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'telephone': [50.9452736318408,\n",
       "  50.646766169154226,\n",
       "  50.9452736318408,\n",
       "  52.039800995024876,\n",
       "  52.33830845771144,\n",
       "  52.83582089552239,\n",
       "  54.22885572139303,\n",
       "  52.83582089552239,\n",
       "  53.333333333333336,\n",
       "  53.53233830845771,\n",
       "  53.333333333333336,\n",
       "  54.42786069651741,\n",
       "  53.134328358208954,\n",
       "  54.62686567164179,\n",
       "  54.32835820895522,\n",
       "  54.22885572139303,\n",
       "  54.42786069651741,\n",
       "  54.12935323383085,\n",
       "  53.53233830845771,\n",
       "  53.73134328358209],\n",
       " 'fiction': [50.25125628140704,\n",
       "  52.06030150753769,\n",
       "  52.462311557788944,\n",
       "  52.462311557788944,\n",
       "  51.959798994974875,\n",
       "  52.06030150753769,\n",
       "  52.96482412060301,\n",
       "  52.663316582914575,\n",
       "  53.869346733668344,\n",
       "  54.07035175879397,\n",
       "  54.472361809045225,\n",
       "  54.2713567839196,\n",
       "  54.2713567839196,\n",
       "  55.27638190954774,\n",
       "  55.879396984924625,\n",
       "  55.27638190954774,\n",
       "  56.28140703517588,\n",
       "  54.77386934673367,\n",
       "  56.28140703517588,\n",
       "  56.18090452261306],\n",
       " 'slate': [46.007984031936125,\n",
       "  47.80439121756487,\n",
       "  47.10578842315369,\n",
       "  47.0059880239521,\n",
       "  48.30339321357285,\n",
       "  48.203592814371255,\n",
       "  48.60279441117765,\n",
       "  48.203592814371255,\n",
       "  48.902195608782435,\n",
       "  48.003992015968066,\n",
       "  48.003992015968066,\n",
       "  47.604790419161674,\n",
       "  48.003992015968066,\n",
       "  47.30538922155689,\n",
       "  47.405189620758485,\n",
       "  48.40319361277445,\n",
       "  46.50698602794411,\n",
       "  47.0059880239521,\n",
       "  48.10379241516966,\n",
       "  47.604790419161674],\n",
       " 'government': [52.36220472440945,\n",
       "  54.13385826771653,\n",
       "  54.625984251968504,\n",
       "  55.511811023622045,\n",
       "  55.610236220472444,\n",
       "  56.2007874015748,\n",
       "  55.70866141732284,\n",
       "  55.90551181102362,\n",
       "  56.79133858267716,\n",
       "  56.69291338582677,\n",
       "  56.988188976377955,\n",
       "  55.41338582677165,\n",
       "  56.10236220472441,\n",
       "  55.21653543307087,\n",
       "  55.118110236220474,\n",
       "  55.01968503937008,\n",
       "  54.625984251968504,\n",
       "  54.82283464566929,\n",
       "  54.330708661417326,\n",
       "  52.854330708661415],\n",
       " 'travel': [49.490835030549896,\n",
       "  51.22199592668024,\n",
       "  52.240325865580445,\n",
       "  53.05498981670061,\n",
       "  54.378818737270876,\n",
       "  54.073319755600814,\n",
       "  55.60081466395112,\n",
       "  55.091649694501015,\n",
       "  54.58248472505092,\n",
       "  54.989816700611,\n",
       "  54.276985743380855,\n",
       "  54.276985743380855,\n",
       "  54.276985743380855,\n",
       "  53.86965376782077,\n",
       "  54.073319755600814,\n",
       "  54.4806517311609,\n",
       "  53.258655804480654,\n",
       "  53.15682281059063,\n",
       "  53.360488798370675,\n",
       "  52.64765784114053]}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_gen_eval(genre_data = mul_val):\n",
    "    gens = genre_data['genre'].unique()\n",
    "    gen_acc = {}\n",
    "    for gen in gens:\n",
    "        x_gen_acc = {}\n",
    "        for x_gen in gens:\n",
    "            if x_gen == gen:\n",
    "                continue\n",
    "            gen_data = (genre_data[genre_data['genre'] == x_gen]).drop(['genre'], axis=1)\n",
    "            premise = []\n",
    "            hyp = []\n",
    "            target = []\n",
    "            for i in range(len(gen_data)):\n",
    "                premise.append(gen_data.iloc[i]['sentence1'].split())\n",
    "                hyp.append(gen_data.iloc[i]['sentence2'].split())\n",
    "                target.append(gen_data.iloc[i]['label'])\n",
    "            labels = []\n",
    "            for label in target:\n",
    "                labels.append(label_dcit[label])\n",
    "            token = VocabDataset(premise, hyp, labels, char2id)\n",
    "            gen_loader = torch.utils.data.DataLoader(dataset=token, batch_size=BATCH_SIZE,\n",
    "                                                       collate_fn=collate_func, shuffle=True)\n",
    "            my_mod = Fc_GRU(hidden_size=800, num_layers=1, num_classes=3, Fc_dim = 300, dropout=True, interact=True)\n",
    "            my_mod = my_best_gru.to(device)\n",
    "            my_mod.load_state_dict(torch.load( 'best_GRU_4_' + gen + '.pth'))\n",
    "            acc, _ = test_GRU_model(gen_loader , my_mod)            \n",
    "            x_gen_acc[x_gen] = acc\n",
    "        gen_acc[gen] = x_gen_acc\n",
    "    return gen_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen_val = x_gen_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': {'telephone': 49.25373134328358,\n",
       "  'slate': 47.50499001996008,\n",
       "  'government': 49.803149606299215,\n",
       "  'travel': 49.287169042769854},\n",
       " 'telephone': {'fiction': 52.06030150753769,\n",
       "  'slate': 47.80439121756487,\n",
       "  'government': 51.27952755905512,\n",
       "  'travel': 50.91649694501018},\n",
       " 'slate': {'fiction': 51.05527638190955,\n",
       "  'telephone': 51.54228855721393,\n",
       "  'government': 52.16535433070866,\n",
       "  'travel': 51.12016293279022},\n",
       " 'government': {'fiction': 50.95477386934673,\n",
       "  'telephone': 50.44776119402985,\n",
       "  'slate': 47.70459081836327,\n",
       "  'travel': 51.62932790224033},\n",
       " 'travel': {'fiction': 51.65829145728643,\n",
       "  'telephone': 51.243781094527364,\n",
       "  'slate': 47.405189620758485,\n",
       "  'government': 54.52755905511811}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gen_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
